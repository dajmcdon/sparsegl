---
documentclass: jss
author:
  - name: Daniel McDonald
    affiliation: |
      | University of British Columbia
    address: |
      | Department of Statistics
      | Vancouver, BC Canada
    email: \email{daniel@stat.ubc.ca}
    url: \url{https://dajmcdon.github.io/}
  - name: Xiaoxuan Liang
    affiliation: 'University of British Columbia \AND'
    address: |
      | Department of Statistics
      | Vancouver, BC Canada
  - name: Aaron Cohen
    affiliation: 'Indiana University'
    address: |
      | Department of Statistics
      | Bloomington, IN USA
title:
  formatted: "Estimating Sparse Group Lasso with the \\pkg{sparsegl} R Package"
  # If you use tex in the formatted title, also supply version without
  plain:     "Sparse Group Lasso with \\pkg{sparsegl}"
  # For running headers, if needed
  short:     "\\pkg{sparsegl}: Sparse Group Lasso"
abstract: >
  The sparse group lasso is a high-dimensional regression technique that is
  useful for problems whose covariates have a naturally grouped structure, and
  where sparsity is encouraged at both the group and individual covariate
  level. In this paper we discuss two algorithms for this optimization
  problem, as well as their implementation in Fortran as part of a new R
  package. This R package is an advance over existing packages, as it is the
  only one that solves this problem in a sufficiently fast computation time
  for truly high-dimensional situations.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
  \input{jss-sparsegl-preamble}
output: rticles::jss_article
bibliography: [sparsegl.bib, pkgs.bib]
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, fig.path = "fig/", autodep = TRUE)
options(prompt = 'R> ', continue = '+ ')
# Add any packages we want to cite to the list below. Don't edit the bib
# manually, or changes will be overwritten. Add other references to
# `sparsegl.bib`
# 
# Note: tags are R-<pkg>
knitr::write_bib(c("devtools", "knitr", "testthat", "usethis", "rticles",
                   "SGL","gglasso", "msgl", "biglasso", "microbenchmark", "ggplot2"),
                 file = "pkgs.bib")
```

# Introduction

\nocite{R-gglasso,R-SGL,R-msgl}

* Introduction to the package what does it do. 1-2 paragraphs.

* Description of related packages and what they do and don't do. 3-4 paragraphs.

* Paragraph outlining the rest of the paper.

Linear regression is the most common type of statistical model for fitting and prediction. When solving the high-dimensional learning problems, adding a weighted regularization component helps to reduce the chances of overfitting and improve the model performance to unseen data. More specifically, the lasso is an $l1$-based penalty, which encourages the sparsity in the solution and brings about significant computational advantages. Group-lasso penalty is an enhanced approach to variable selection, preserving the grouping structure but only achieving the sparsity at the group level. To attain the sparsity at both group and individual feature levels, the sparse group-lasso penalty is introduced, namely a linear convex combination of the lasso penalty and group lasso-penalty. In this case, the package is developed regarding the class of sparse group-lasso problems, where the objective function is the sum of mean squared deviation and the sparse group-lasso penalty. This package focuses on finding the optimal solutions to the sparse group-lasso penalized learning problems at a sequence of regularization parameters, and more importantly, how to improve the computational efficiency in the meanwhile.

Many packages have been developed to perform group-lasso in different programming languages. In the same environment R, there are already other existing \pkg{SGL}, \pkg{R} packages and \pkg{gglasso},\pkg{biglasso}. All of them are with strengths but also certain limitations to some extent: the SGL package has sparsity, but does direct coordinate descent and does not apply a heuristic like the strong rule, so it does not take advantage of such a computational speedup. Because of this, \pkg{SGL} is too slow to be of use in analyzing the high-dimensional problems that occur commonly, for example in genetic applications. The \pkg{gglasso} and \pkg{biglasso} \citep{zeng2020biglasso} packages, on the other hand, both are computationally fast. The former incorporates the strong rule, and the latter involves a hybrid safe-strong rule and various procedures such as scalable storage, parallel implementation in C++ and R levels to manage data-larger-than-RAM cases, realizing memory efficiency as well. Nevertheless, they do not incorporate within-group sparsity (i.e., it performs group lasso, not sparse group lasso). When this package is used on a grouped dataset, the solution will have some active groups and some inactive, but within an active group, generally all the coefficients will be nonzero. 

\pkg{msgl} \citep{vincent2014sparse} is also an R package solving sparsel group lasso multiclassification problems via fitting multinomial logistic regression model. This package overcomes the nondifferential penalty problem at zero and since the algorithm is Newton-based, it substitutes the Hessian matrix with an upper bound on that, which reduces the costs of computation. However, the multinomial logistic regression is an extension to the binary logistic regression, allowing for taking the response variable with multiple classes, which consumes an excessive amount of time for model fitting when processing binary response variable.

In the environment Python, \pkg{asgl} \citep{civieta2020adaptive} proposed an adaptive sparse group-lasso, which improves the variable selection accuracy by flexibly adjusting the weights in the penalization on the groups of features in terms of the feature importance. Another breakthrough of this package is that the loss function being penalized is in the framework of quantile regression, inducing two dimensionality-reduction techniques, Principal Component Analysis and Partial Least Squares for tackling high-dimensional optimization problems. However, one drawback of this package is that it could be time-consuming to use grid-search for hyper-parameters tuning process.

The paper \citep{ida2019fast} also brings up a fast algorithm applying KKT and revised KKT checks repetitively through block coordinate descent for computing sparse group-lasso model. In this algorithm, the main feature is that the revised KKT check first searches through all groups to filter out the groups whose feature coefficients are zero, then the original KKT check confirms the actual nonzero coefficient groups among the remaining candidate active groups. The revised KKT check could remarkably reduce the time complexity, which is introduced by approximating a metric used to compare against a fixed threshold with a tight upper bound.

Our contribution, then, is to provide a package that performs sparse group lasso, and is faster than existing algorithms. The algorithms for this package are written in Fortran, and are based on the algorithm from the gglasso package. In section 2, we describe the algorithm in detail, paying particular attention to the strong rule. In section 3, we show how to use the package, running through an example with simulated data. In section 4, we compare our package to the other existing group-lasso packages, and also compare the two variants of our algorithm with each other. We make a summary and discussions in section 5, and make promising proposals for potential future research as well.


## Code formatting instructions (remove this section later)

In general, don't use Markdown, but use the more precise LaTeX commands instead:

* \proglang{Java}
* \pkg{plyr}

One exception is inline code, which can be written inside a pair of backticks (i.e., using the Markdown syntax).

If you want to use LaTeX commands in headers, you need to provide a `short-title` attribute. You can also provide a custom identifier if necessary. See the header of Section \ref{r-code} for example.

### \proglang{R} code {short-title="R code" #r-code}

Can be inserted in regular R markdown blocks.

```{r}
x <- 1:10
x
```

### Features specific to \pkg{rticles} {short-title="Features specific to rticles"}

* Adding short titles to section headers is a feature specific to \pkg{rticles} (implemented via a Pandoc Lua filter). This feature is currently not supported by Pandoc and we will update this template if [it is officially supported in the future](https://github.com/jgm/pandoc/issues/4409).
* Using the `\AND` syntax in the `author` field to add authors on a new line. This is a specific to the `rticles::jss_article` format.

### Additional formatting help

[bookdown](https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html)




# Methodology, estimation and prediction

The lasso is ``a regression analysis method that performs both variable selection and regularization,'' which uses an $l1$ penalty \citep{tibshirani1996regression} to select a subset of predictors to be nonzero. The minimization problem using linear regression model is of the form
\[
\min_{\vbeta} \frac{1}{2}\norm{\vY-\mX\vbeta}_2^2 + \lambda \norm{\vbeta}_1,
\]
where $\mX$ is an $n$-by-$p$ data matrix, $\beta$ is the unknown coefficient vector, and $\lambda$ is a hyperparameter that enforces the regularization/variable selection. In this problem it is assumed that the response $\vY$ is linearly related to the predictors (the columns of the data matrix $\mX$), with the goal being to estimate the coefficient vector $\beta$ responsible for this linear relationship. In general it returns a solution with some nonzero entries in the $\vbeta$ vector, and the rest zero.

A variant of this, the group lasso \citep{yuan2006model} is appropriate when there is a natural grouping structure for the coefficients; that is, we assume that the vector of coeffiecients is partitioned into groups, or subvectors, and, by analogy with regular lasso regression, only a few of the groups are active, i.e., have nonzero coefficients. The group lasso thus performs regularization that has the effect of discarding groups of predictors rather than the predictors themselves; it is of the form 
\[
\min_{\vbeta}\frac{1}{2}\norm{\vY-\sum_{l=1}^m\mX^{(l)}\vbeta^{(l)}}_2^2 + \lambda\sum_{l=1}^m\sqrt{p_l}\norm{\vbeta^{(l)}}_2.
\]
Note that the grouping structure is explicitly stated in the above equation: the vector of coefficients, $\vbeta$, is thought of as a concatination of the coefficient subvectors of the various groups $\vbeta^{(l)}$, and similarly the data matrix $\mX$ is the concatination of submatrices, each submatrix $\mX^{(l)}$ being composed of the columns that correspond to the particular group. Thus the first part of the equation, $\vY-\sum_{l=1}^m\mX^{(l)}\vbeta^{(l)}$, is identical to the more simply-written equation $\vY-\mX\vbeta$, but is written with this partition in mind. 

The second part of the equation, however---$\sum_{l=1}^m\sqrt{p_l}\norm{\vbeta^{(l)}}_2$---is different than the corresponding part in the original lasso equation. Rather than being an $l1$-norm of the vector $\vbeta$, it is a sum of the (non-squared) $l2$-norms of the coefficient vectors of the various groups. It is interesting to note that it is the non-differentiability of this expression at $\mathbf{0}$ that accounts for the group-discarding property of the problem, similar to how the $l1$-norm's non-differentiability at $\mathbf{0}$ is responsible for the discarding property of coefficients in the original lasso. 

As with the original lasso equation, there is only a single tuning parameter $\lambda$, whose value determines the strength of regularization. Within the second summation are the relative weights of the groups, $p_l$, though these are usually determined by the size of the corresponding groups, and so in the rest of this paper this notation is suppressed from the equations.

Finally, in a group-structured problem as above, it may be desirable to enforce sparsity, not only among the groups, but also within the groups. The sparse group lasso \citep{simon2013sparse} does this by solving the following minimization problem
\begin{equation}
  \label{eq:sparsegl}
\min_{\vbeta}\frac{1}{2}\norm{\vY-\sum_{l=1}^m\mX^{(l)}\vbeta^{(l)}}_2^2 + (1-\alpha)\lambda\sum_{l=1}^m||\vbeta^{(l)}||_2+\alpha\lambda||\vbeta||_1.
\end{equation}
This equation is very similar to that of the group lasso, and in fact one sees that the only difference is that it adds a second regularization term, the overall $l1$-norm of the full vector $\vbeta$, which we first saw in the original lasso formulation.  There is now a second tuning parameter $\alpha$, which controls the relative emphasis of intra- vs inter- sparsity in the predictors.

The sparse group lasso, like the other two lassos, does not have a closed form solution and thus relies on computational algorithms, which may be very intensive for large datasets and/or many values of $\lambda$. The rest of this paper therefore concerns the mitigation of this issue, in particular the following question: if we are solving the sparse group lasso, not for a single value of the tuning parameter $\lambda$ but a whole parameter space---in this context, a range of values $\{\lambda_1,\dots \lambda_S\}$--- can we shorten the total computational time by making use of the already-computed solution at previous $\lambda$s' in the parameter space to speed up computation at a given $\lambda$?

In this paper we make use of a heuristic called the Strong Rule \citep{tibshirani2012strong} to help solve this problem, with some success. The strong rule uses the solution at the previous $\lambda_{s-1}$ to predict which groups will remain inactive upon solution at the current $\lambda_s$, and discards those groups before entering into the algorithm. If a significant number of groups are thrown out before entering the algorithm, convergence time can improve significantly. The \texttt{R} package we have created has an algorithm called \textbf{four-step algorithm}, which uses the strong rule. 

## Overview of the algorithm

There is no closed-form solution to the optimization problem in \autoref{eq:sparsegl} above, so we need a numerical algorithm to find the optimal solution. The problem defined by \autoref{eq:sparsegl} is convex, since the expression is a sum of convex functions and the feasable set is convex, so there is an optimal solution, and a variety of methods may be used one (?).

Our package includes an algorithm to accomplish this, which we refer to as the \textbf{four-step} algorithms. The general framework for our algorithm is based on a block-wise coordinate descent algorithm (see \citep{yang2015fast, simon2013sparse}). What this means is that we loop over the groups and, for a given group, update only those variables while holding all other groups constant. In particular, we will use a majorization-minimization coordinate descent scheme; the term 'majorization' comes from the idea that, instead of using the exact equation to determine the step size and direction in every update step, we update according to a simpler expression than \autoref{eq:sparsegl} that majorizes, or bounds from above, the expression we want to solve for. This is explained in more detail in the following paragraphs.

For the rest of this section, we will focus on a particular group $k$. We hold the coefficients for all other groups fixed, and find an appropriate update step for group $k$. We note here that, because the loss function in \autoref{eq:sparsegl} is differentiable and the penalty terms are convex and separable (i.e., they can be decomposed into a sum of functions each only involving a single group), a coordinate descent algorithm is guaranteed to converge to a global optimum \citep{tseng2001convergence}.

To begin with, we will introduce some notation. Let 

\[
\vR_{(-k)} = \vY - \sum_{l \neq k} \mX^{(l)} \vbeta^{(l)}
\]
be the partial residual of $\vY$, where all the group fits besides that of group $k$ are subtracted from $\vY$. Since all groups besides group $k$ are held fixed, we don't need to consider the penalty terms corresponding to those fixed groups. So what we are really interested in minimizing is the following:

\begin{equation}
	\label{eq:sparsegroupk}
\frac{1}{2n} \norm{\vR_{(-k)}-\mX^{(k)}\vbeta^{(k)}}_2^2 + (1-\alpha)\lambda \norm{\vbeta^{(k)}}_2 + \alpha \lambda \norm{\vbeta^{(k)}}_1. 
\end{equation}
For ease of notation, we will suppress the superscript $(k)$ from $\vbeta^{(k)}$, with the understanding that we are really referring to only the $k$-th group of the coefficient vector. We will also define, using these terms, the unpenalized loss function

\[
\ell (\vR_{(-k)}, \vbeta) = \frac{1}{2n}\norm{\vR_{(-k)} - \mX^{(k)}\vbeta}_2^2, 
\]
so that our optimization problem for the $k$-th group becomes $\ell (\vR_{(-k)},\vbeta) + (1-\alpha)\lambda\norm{\vbeta}_2 + \alpha \lambda \norm{\vbeta}_1$, and we are interested in finding an optimal value, $\hat{\vbeta}$.

Any global minimum is determined by the subgradient equation, which is similar to a first-derivative test for minima, except the $\norm{\cdot}_2$ and $\norm{\cdot}_1$ penalty terms are non-differentiable at $\mathbf{0}$; because of this, the (sub)differential for each coordinate in $\vbeta$ (i.e., $\vbeta^{(k)}$) is not a single value, but a range of possible values. For \autoref{eq:sparsegroupk} above, taking the subdifferential and setting equal to zero gives us the following condition that the minimum $\hat{\vbeta}$ needs to satisfy: 

\begin{equation}
  \label{eq:subgrad}
\frac{1}{n}\mX^{(k)\top}(\vR_{(-k)} - \mX^{(k)}\vbeta) = (1-\alpha)\lambda \mathbf{u} + \alpha \lambda \mathbf{v},
\end{equation}
where $\mathbf{u}$ is the subgradient of $\norm{\vbeta}_2$ and $\mathbf{v}$ is the subgradient of $\norm{\vbeta}_1$. The first is defined to be $\frac{\vbeta}{\norm{\vbeta}_2}$ if $\vbeta$ is a nonzero vector, and is any value in the set $\{\mathbf{u} : \norm{\mathbf{u}}_2 \leq 1 \}$ otherwise; the second, $\mathbf{v}$, is defined coordinate-wise as $v_j = \text{sign}(\beta_j)$ if $v_j \neq 0$, and is any value $v_j \in \{v_j : |v_j| \leq 1 \}$ otherwise.


At this point, it is possible to use \autoref{eq:subgrad} to derive an update step and develop a cyclical coordinate-wise algorithm, but it is too computationally expensive, involving repeated large-matrix multiplication (\citep{simon2013sparse}); instead we use the majorization-minimization idea discussed above.

Since the unpenalized loss $\ell (\vR_{(-k)}, \vbeta)$ is a quadratic function in $\vbeta$, it is equal to its second order Taylor expansion about any point $\vbeta_0$ in the parameter space---indeed, this is the definition of a function being quadratic. We thus start with the following equality for any given $\vbeta_0$ (recall that $\vbeta_0 = \vbeta_0^{(k)}$):  

\begin{equation}
\forall \vbeta,\ \ell(\vbeta) = \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\top\nabla \ell(\vbeta_0)+\frac{1}{2}(\vbeta - \vbeta_0)^\top \Hessian (\vbeta - \vbeta_0),
\label{eq:TaylorExp}
\end{equation}

where the gradient $\nabla \ell$ is the first total derivative of $\ell$ (evaluated at $\vbeta_0$) and $\Hessian$, the Hessian, is the second total derivative. Note that we have again suppressed notation, $\ell(\vbeta) = \ell(\vR_{(-k)},\vbeta)$, since $\vbeta = \vbeta^{(k)}$ is the only variable. Recall that $\nabla$ is the vector of first partial derivatives of $\ell$ with respect to each coordinate of $\vbeta$, evaluated at $\vbeta_0$, and $\Hessian$ is a $p$-by-$p$ matrix of mixed partial derivatives of $\ell$. In this particular case, a short computation shows that the Hessian is simply written in terms of $\mX$: $\Hessian = \mX^\top \mX$.

The matrix $\mX$ is, for the problems we are interested in solving, very large, and so computing $\mX^\top \mX$, storing it in memory, and inverting, is going to be an issue, especially since we will be doing update steps many times. What we can do, and will do, is replace this matrix with a much simpler one, $t\mathbf{I}$, a diagonal matrix with the value of $t$ selected to be such that this dominates the Hessian (in the sense that $t\mathbf{I} - \Hessian$ is positive definite). For our algorithm we choose the largest eigenvalue of the Hessian and use that for $t$. We get the following inequality:



\begin{equation}
\forall \vbeta,\vbeta_0,\ \ell(\vbeta) \leq \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\top\nabla \ell(\vbeta_0)+\frac{1}{2t}(\vbeta - \vbeta_0)^\top  (\vbeta - \vbeta_0).
\label{eq:dominate}
\end{equation}

We take the original minimization problem (\autoref{eq:sparsegroupk}) and replace the loss $\ell$ with the right-hand side of \autoref{eq:dominate}. Thus \autoref{eq:sparsegroupk} is now dominated (majorized) by the following expression
\begin{equation}
\label{eq:Meq}
M(\vbeta) = \ell(\vbeta_0)+(\vbeta - \vbeta_0)^\top\nabla \ell(\vbeta_0)+\frac{1}{2t}\norm{\vbeta_0-\vbeta}_2^2+ (1-\alpha)\lambda\norm{\vbeta}_2+\alpha\lambda\norm{\vbeta}_1,
\end{equation}
which no longer involves the large matrix multiplication that using the original loss function would have confronted us with. 

The optimal value $\hat{\vbeta}$ for $M(\vbeta)$ is determined by its subgradient equation, which is similar to that of \autoref{eq:subgrad}:

\begin{equation}
\frac{1}{t} (\vbeta - (\vbeta_0 - t\nabla \ell)) +(1-\alpha)\lambda \mathbf{u} + \alpha \lambda \mathbf{v} = \mathbf{0},
\end{equation}
with $\mathbf{u}$ and $\mathbf{v}$ as before. If we solve this for $\vbeta$ in terms of $\vbeta_0$, paying close attention to the conditions for the subdifferentials when $\vbeta$ is and isn't $\mathbf{0}$, we get the following equation:
\begin{equation}
\hat{\vbeta} = U(\vbeta_0) =
\left(1-\frac{t(1-\alpha)\lambda}{\norm{S(\vbeta_0-t\nabla \ell(\vbeta_0),\ t\alpha\lambda)}_2}\right)_+ S(\vbeta_0-t\nabla \ell(\vbeta_0),t\alpha\lambda),
\label{eq:updateStep}
\end{equation}
where $S$ is the coordinate-wise soft threshold operator, defined by a vector $\boldsymbol\gamma$ and scalar $b$ according to
\begin{equation}
\label{softthresh}
(S(\boldsymbol\gamma,b))_j = \text{sign}(\gamma_j)(|\gamma_j| - b)_+,
\end{equation}
i.e., for each coordinate in the vector, it shrinks that coordinate in magnitude by the amount $b$, and sets it to zero if the magnitude of that coordinate was smaller than $b$ to begin with.



\autoref{eq:updateStep} defines the update step at a given group $k$, via $\vbeta \leftarrow U(\vbeta)$. At a point $\vbeta_{old} = \hat{\vbeta}^{(k)}_{old}$, we find the convex function $M$ that majorizes the objective function--centered at that $\vbeta_{old}$--and choose the new value of beta to be the minimizer of majorized function, $\vbeta_{new} = U(\vbeta_{old})$. This update step has the advantage that it is easy to compute, and is guaranteed to converge to the optimum $\hat{\vbeta} = \hat{\vbeta}^{(k)}$ for that given group.  


By examining the update step we see that it is possible for the whole group to be set to zero (made inactive) because of the threshold operator $(\cdot)_+$ in the first part of the expression, and it is also possible for individual components of $\vbeta^{(k)}$ to be zeroed out from the coordinate-wise threshold $S(\cdot)$. From this it is seen that this update step tends to enforce sparsity at both the group and individual level.

Both of the algorithms in the \pkg{sparsegl} package use the update step $U$ in \autoref{eq:updateStep}, iterating over all the groups and performing one update per pass. Since the partial residual $\vR_{(-k)}$ is used in $U$ (via $\ell$), we note that, for a given group $k$ at a given point in the cycle, the update $\vbeta^{(k)} \leftarrow U(\vbeta^{(k)})$ uses the new values of $\{\vbeta^{(1)}, \dots \vbeta^{(k-1)}\}$ and the old values for the subsequent $\{\vbeta^{(k+1)},\dots \vbeta^{(m)}\}$.

Because the optimization problem is convex, with differentiable loss and separable penalty, it can be shown that this type of blockwise coordinate descent algorithm is guaranteed to converge to the optimal solution for the full vector $\hat{\vbeta}$. That is, it does not get `stuck' at any inflection points or local minima. Therefore, if that was all the algorithm did, there would be no need to check the KKT conditions for optimality. 

However, our algorithms both make use of the sequential strong rule, a heuristic that predicts, before computation, which groups will remain inactive, and discards them; this results in significant computational savings, but because this pre-processing prediction is not always correct, we will need to do a KKT check \citep{boyd2004convex} after convergence to make sure the initial prediction was accurate. This is described in the next section.

## KKT Check and the Strong Rule

As mentioned above, the strong rule \citep{tibshirani2012strong} is easy and fast to perform, discarding large numbers of predictors before computation, but it is not guaranteed to be correct, so it becomes necessary to double-check the solution after convergence, to make sure active groups were not accidentally discarded. 

The version of the strong rule used here is called the \textbf{sequential strong rule}; it makes use of the fact that we are solving for a sequence $\{\lambda_1 \geq \lambda_2 \geq \dots \geq\lambda_S\}$ of lambda parameters, rather than a single value. At each $\lambda_s$, we rely on the fact that we have solved the problem for the previous $\lambda_{s-1}$, and use this information to quickly discard many predictors. Without loss of generality, for the rest of this section assume that the problem has already been solved for $\lambda_{s-1}$.



The motivation for the strong rule comes from the KKT stationarity condition, which is similar to the first derivative test for minima \citep{boyd2004convex}. Explicitly, this is the condition that the subdifferential equations (see \autoref{eq:subgrad}) are satisfied at the proposed minimum $\hat{\vbeta} =\hat{\vbeta}(\lambda_s)$. This is the necessary and sufficient condition for a global optimum. Note that we will be emphasizing the dependence of $\hat{\vbeta},\vR_{(-k)}$, and so forth, on $\lambda$ in this section.
 
From \autoref{eq:subgrad} we derive an inequality that is equivalent to the KKT stationarity condition in the case that $\hat{\vbeta}^{(k)} = \mathbf{0}$. Because the algorithm is guaranteed to converge to the optimum for all groups on which it is run, we only need to check the condition for the discarded groups, i.e., those that we have preassigned $\hat{\vbeta}^{(k)} = \mathbf{0}$ (at a given $\lambda$). 

First, if $\hat{\vbeta}^{(k)} = \mathbf{0}$, the subgradient equations reduce to the following:

\begin{equation}
\label{eq:subgradreduced}
\mX^{(k)\top}\vR_{(-k)}/n - \alpha \lambda \mathbf{v} = (1-\alpha) \lambda \mathbf{u},
\end{equation}
where $\mathbf{u}$ and $\mathbf{v}$ are the subdifferentials of $\norm{\vbeta^{(k)}}_2$ and $\norm{\vbeta^{(k)}}_1$, as in section 2.1. We claim that if the following inequality

\begin{equation}
\label{eq:kkt}
\norm{S(\mX^{(k)\top}\vR_{(-k)}/n,\ \alpha\lambda)}_2 \leq (1-\alpha)\lambda
\end{equation}
is satisfied (with $S(\cdot)$ the soft-threshold operator from section 2.1), then $\hat{\vbeta}^{(k)} = \mathbf{0}$ is the optimum for that group at that particular $\lambda$. In other words, this inequality is equivalent to the KKT stationarity condition for inactive groups.

The proof is as follows: denote $\boldsymbol\gamma = \mX^{(k)\top}\vR_{(-k)}/n$, and define $v_j = \text{sign}(\gamma_j)$ if $|\gamma_j| > \alpha \lambda$, and $v_j = \frac{\gamma_j}{\alpha \lambda}$ otherwise. Then 
\[
\boldsymbol\gamma - \alpha \lambda \mathbf{v} = S(\boldsymbol\gamma,\alpha \lambda).
\]
Next, define $\mathbf{u} = S(\boldsymbol\gamma,\alpha \lambda)/(1-\alpha)(\lambda)$. From these two definitions it follows that \autoref{eq:subgradreduced} is satisfied, but not necessarily by elements of the subdifferentials. But if \autoref{eq:kkt} is also satisfied, then $\boldsymbol{u}$ is a subgradient of $\norm{\vbeta^{(k)}}_2$, and $\boldsymbol{v}$ a subgradient of $\norm{\vbeta^{(k)}}_1$, evaluated at $\vbeta^{(k)}=0$.

We have derived the KKT condition for the $k$-th group to be $0$. We will now derive and explain the strong rule, which is based on this inequality. At this point it will be helpful to make explicit the dependence of these expressions on $\lambda$. First, for each group $k$, we define a function of $\lambda$:

\begin{equation}
c_k(\lambda) = S(\mX^{(k)\top}\vR_{(-k)}(\lambda)/n,\ \alpha\lambda).
\end{equation}
So for example \autoref{eq:kkt} becomes $\norm{c_k(\lambda)}_2 \leq (1-\alpha)\lambda$. There is a different function for every group $k$. Note that, since $\vR_{(-k)}$ involves $\hat{\vbeta}$, we needed to have actually solved the optimization problem at $\lambda$ in order to compute $c_k(\lambda)$.

It is natural to consider the properties such a function would have. These functions are continuous, for example, as the solution to the optimization problem, \autoref{eq:sparsegl} varies continuously with its tuning parameter $\lambda$. We are now going to make an assumption about $c_k$, which seems unintuitive at first, and in fact is not always true, but is true often enough to be useful: we assume that $c_k(\lambda)$ is $(1-\alpha)$-Lipschitz, i.e., that
\begin{equation}
\label{eq:lipschitz}
\forall \lambda,\ \lambda^{\prime}\ \  \ \norm{c_k(\lambda)-c_k(\lambda^{\prime})}_2 \leq (1-\alpha)\norm{\lambda - \lambda^{\prime}}_2.
\end{equation}
This is essentially saying that $c_k$ is not only continuous but does not vary 'too fast'; this is equivalent to saying that the function is differentiable almost everywhere and has bounded derivative.


We can finally define the \textbf{strong rule} check for discarding groups. Assume that the optimization problem has been solved for $\lambda_{s-1}$; then if the inequality
\begin{equation}
\label{eq:strong}
\norm{c_k(\lambda_{s-1})}_2 \leq (1-\alpha)(2\lambda_s - \lambda_{s-1})
\end{equation}
holds, then we discard group $k$ from the problem, before solving for $\lambda_s$. That is to say, when we move from $\lambda_{s-1}$ to $\lambda_s$, we first run this check on all groups, using the previously computed solution for $\hat{\vbeta}(\lambda_{s-1})$, and then move on to coordinate descent, using only those groups that failed this inequality.

This discarding rule is fast, using the previously computed solution and a simple inequality, and in practice it tends to accurately discard large numbers of groups. The reason why it works is as follows: assuming the Lipschitz condition on $c_k(\lambda)$, then if the strong rule is satisfied for $\lambda_s$, we get by the triangle inequality that
\begin{align*}
\norm{c_k(\lambda_s)}_2 &\leq \norm{c_k(\lambda_s) - c_k(\lambda_{s-1})}_2 + \norm{c_k(\lambda_{s-1})}_2 \\
&\leq (1-\alpha)(\lambda_{s-1} - \lambda_s) + (1-\alpha)(2\lambda_s - \lambda_{s-1})
=(1-\alpha)\lambda_s,
\end{align*}
which is exactly the KKT condition required for $\hat{\vbeta}^{(k)}(\lambda_s) = 0$, \autoref{eq:kkt}.

Note that the upper bound for the strong rule condition, the right-hand side of \autoref{eq:strong}, is smaller than the upper bound for the KKT condition inequality, i.e., $(1-\alpha)(2\lambda_s - \lambda_{s-1}) < (1-\alpha)(\lambda_{s-1})$, since $\lambda_{s} < \lambda_{s-1}$. We see from this that the strong rule condition, which is a condition on the subdifferential at $\lambda_{s-1}$, is stronger than the zero KKT condition at $\lambda_{s-1}$; the logic of the strong rule is that, if group $k$ satisfies the zero KKT condition at $\lambda_{s-1}$, with some wiggle room in the inequality, and if the function $c_k(\lambda)$ which characterizes the zero KKT condition does not change too much going from one lambda to the next, then the zero KKT will be satisfied for the next value $\lambda_s$.

Finally, we should reiterate that it is possible for the strong rule to make a mistake, the Lipschitz assumption is just that, an assumption. Because of this, it is critical that, after discarding some of the groups and running the algorithm on the others, the zero KKT condition is checked on all those discarded groups to make sure that the strong rule made the right decision, and that those groups are actually zero. Violators of the KKT condition should be put in 'active' status and the algorithm re-run with those groups included. The details of this are in the next section.

Now that we understand the KKT check and strong rule check on groups, we briefly discuss some notation which is used in the next section. In this section we defined a function $c_k(\lambda)$ that was parameterized by the group $k$ and taking in as an argument $\lambda$. We will change emphasis and define a function:
\[
\mathcal{S}_{\lambda}(A),
\]
that takes in a subset of the set of groups, $A$, and returns the set of all elements of $A$ which \textbf{fail} the strong rule check at $\lambda$. This is precisely the set of groups that we suspect of being active, that we should run the update step on. The reason why we want this notation is that it is a waste of time to run the strong rule check on those already known or suspected to be active. 

Similarly, we will define the function 
\[
KKT_{\lambda}(A)
\]
that takes in a  set of groups, and returns the subset of those elements of $A$ that \textbf{fail} the 0-KKT check, i.e. those elements $k \in A$ for which $\norm{c_k(\lambda)}_2 > (1-\alpha)\lambda$. Again, we do not need to check the KKT conditions for those groups on which the update step was run until convergence, so we will want to specify the subset of groups that we need to run the KKT check on after convergence of $U$.

## Logistic regression 
This package is also extended for solving optimization problems with if the response variable is binary. The objective function is changed to the sum of logistic loss and the combination of $l1$ and $l2$- penalties: 
$$
\min_{\vbeta\in\mathbb{R}^p}\left(\frac{1}{2n}\sum_{i=1}^n \log\left(1 + \exp\left(-\vY_i\vbeta^\top \mX_i\right)\right) + (1-\alpha)\lambda\sum_l \rVert\vbeta^{(l)}\rVert_2 + \alpha\lambda\rVert\vbeta\rVert_1 \right) 
$$

The gradient is replaced with
$$
\nabla = -\frac{1}{2n} \sum_{i=1}^n \vY_i\mX_i\frac{1}{1 + \exp(\vY_i\vbeta^\top\mX_i)}
$$

with the Hessian matrix $\Hessian = \frac 14 \mX^\top \mX$.

# The four-step algorithm

The algorithm called four-step algorithms in our package performs the sparse group lasso. This algorithm uses the majorized coordinate-descent update step $U$ from section 2.1, and the order of when it stores the active and potentially active groups guarantees the robustness when speeding up the computation. Before discussing it in detail, we need some notations and explanations.


First, recall from the end of the previous section that $\mathcal{S}_{\lambda}(A)$ is a function which performs the strong rule check on a set of groups $A$ at $\lambda$, and returns the set $A^{\prime}$ of groups in $A$ that fail the strong rule check. For each individual group in the set $A$, it checks the inequality in \autoref{eq:strong}. Recall that the strong rule check at a given $\lambda_k$ actually involves both $\lambda_{k}$ and $\lambda_{k-1}$; this is suppressed in this notation, with the consensus.

Similarly, the function $\textrm{KKT}_{\lambda} (A)$ performs the KKT check described in the previous section, and returns the subset $A^{\prime}$ of groups that fail the KKT check. The idea for both of these functions is to keep track of which groups are potentially active (nonzero) for the given $\lambda$. 

Both of these functions take in only a subset of the groups of coefficients, rather than being run on the whole set each time; this is to save time and reduce redundancy. It would not make sense to run a KKT check on those groups that are already known to be active (for the given $\lambda$), because the coordinate descent algorithm is guaranteed to converge for the groups on which is is run. Similarly for the strong rule check, it would be a waste of time to run that inequality on groups that are already known or suspected of being active, since the point of the strong rule check is to separate those suspected of being active from those that are not, and only updating the coefficient estimates for the suspected active groups.

We will keep track of active/potentially active groups with a set $\mathcal{E}$; this is the set of all groups that have failed the strong rule check, or have failed the KKT check, and it is precisely this set of groups that we perform the update step on until convergence. This set is ever-increasing, that is, if a group fails the strong rule or KKT check at some $\lambda_s$ and is put in $\mathcal{E}$, it stays in $\mathcal{E}$ for every subsequent $\lambda_{s+i}$. It is possible for a group to be active for some value of lambda and then become inactive later on in the lambda path, but this is unlikely, and we are not losing too much efficiency by making this group ever-increasing.

Finally, in the convergence step of the algorithms, we want to keep track of how much $\vbeta$ is moving; if it appears to have found a minimum, and is not moving much from the update step, we want to declare convergence and kick out of the updating step. Because of this, we keep track of the maximum change in the $\vbeta^{(k)}$'s with $\max_{k\in\mathcal{E}} \left(\norm{\hat{\vbeta}^{(k)}_{\{\lambda_i\}} - U\left(\hat{\vbeta}^{(k)}_{\{\lambda_i\}}\right)}\right)$. Once $\max_{k\in\mathcal{E}} \left(\norm{\hat{\vbeta}^{(k)}_{\{\lambda_i\}} - U\left(\hat{\vbeta}^{(k)}_{\{\lambda_i\}}\right)}\right) < \epsilon$, where $\epsilon$ is some pre-determined small amount, we move to the next step.


The highlight of four-step algorithm, \autoref{alg:fourStep} is that instead of adding groups into the 'active' set $\mathcal{E}$ before running the algorithm, the algorithm is first run until convergence on the original set $\mathcal{E}$, and the strong rule is run afterwards. The details are as follows.

\begin{algorithm}[tb!]
  \caption{The four-step Algorithm (revised)}
  \label{alg:fourStep}
  \begin{algorithmic}[1]
    \STATE Initialize $\boldsymbol{\lambda} = \{\lambda_s\}_{s = 1}^n$, and the corresponding coefficients at each $\lambda$ value $\{\hat{\vbeta}_{\{\lambda_s\}}\}_{s = 1}^n$, where $n\in\mathbb{N}$.
    \STATE Let $\epsilon$ denote the convergence tolerance, $\mathcal{E}$ be the active set starting from an empty set, and $k$ indicate the groupings.
    \FOR{$s = 1, 2, \cdots, n$}
    \STATE Run through $\mathcal{E}$ (if not empty) until convergence:
    \bindent
    \WHILE{$\max_{k\in\mathcal{E}} \left(\norm{\hat{\vbeta}^{(k)}_{\{\lambda_s\}} - U\left(\hat{\vbeta}^{(k)}_{\{\lambda_s\}}\right)}\right) >\epsilon$}
    \bindent
    \STATE Update $\hat{\vbeta}^{(k)}_{\{\lambda_s\}}$ by $U\left(\hat{\vbeta}^{(k)}_{\{\lambda_s\}}\right)$ for all $\hat{\vbeta}^{(k)}_{\{\lambda_s\}}\in\mathcal{E}$
    \eindent
    \ENDWHILE 
    \eindent
    \STATE Run strong rule check on inactive groups, then the KKT check on the violators:
    \bindent
    \IF {$KKT_{\lambda_s}(S_{\lambda_s}(\mathcal{E}^c))$}
    \bindent
    \STATE $\mathcal{E} \leftarrow \mathcal{E} \cup KKT_{\lambda_s}(S_{\lambda_s}(\mathcal{E}^c))$
    \STATE Return to Line 4.
    \eindent
    \ENDIF
    \eindent
    \STATE Run KKT rule check on whole inactive set:
    \bindent
    \IF {$KKT_{\lambda_s}(\mathcal{E}^c)$}
    \bindent
    \STATE $\mathcal{E} \leftarrow \mathcal{E}\cup KKT_{\lambda_s}(\mathcal{E}^c)$
    \STATE Return to Line 4.
    \eindent
    \ENDIF
    \eindent
    \ENDFOR
    \RETURN{$\hat{\vbeta} \leftarrow \{\hat{\vbeta}_{\{\lambda_s\}}\}_{s = 1}^n$}
  \end{algorithmic}
\end{algorithm}


Assume we have solved the problem for $\lambda_{s-1}$. We loop over $\mathcal{E}$, which was passed over from $\lambda_{s-1}$, and update until convergence. The difference here is that we do not perform the strong check first. After we have run the update loop until convergence on the active set, we then perform the strong rule on the complement, $\mathcal{E}^c$. Since the strong rule depends on the computation of $\vbeta_{\{\lambda_{s-1}\}}$, it doesn't actually matter whether the coefficients are converged or not. For any groups that fail the strong rule check, we run the KKT check; if there are any violators, put them in $\mathcal{E}$ and go back to step 1. Therefore, the difference is that the condition for adding groups to $\mathcal{E}$ is more stringent and is done after obtaining the converged coefficients. Otherwise, we run the KKT check on the full complement $\mathcal{E}^c$. Again, if there are any violators, we put them in $\mathcal{E}$ and go back to check if coefficients are diverged. If there are no violators, we are done, and we move on to the next $\lambda$ in the sequence.

In this algorithm, we are careful to put potentially active groups into the active set. Instead of putting groups in $\mathcal{E}$ immediately after applying the strong rule, we are hesitant to commit to running the algorithm on the strong rule alone. We run the strong rule after convergence, but then double-check that the suspected 'active' groups are actually so by using the KKT check.


# Example usage
This section provides a simple guidance for how to employ this package for fitting the regularization paths for sparse group-lasso penalized learning problems. For simplicity, we will focus on how to apply the linear regression model when response variable is continuous, and briefly go over the logistic regression case. We first import the sparsegl package, and start with a simulated regression dataset including an input matrix of predictors $\mX$, a real-valued response vector $\vY$, and a vector indicating the grouping structure:
```{r}
library(sparsegl)
```

```{r, data simulation}
set.seed(1010)
n <- 100
p <- 200
X <- matrix(data = rnorm(n*p, mean = 0, sd = 1), nrow = n, ncol = p)
beta_star <- c(rep(5, 5), c(5, -5, 2, 0, 0), rep(-5, 5), c(2, -3, 8, 0, 0), 
               rep(0, (p - 20)))
groups <- rep(1:(p / 5), each = 5)
# simulate data for y in linear regression
eps <- rnorm(n, mean = 0, sd = 1)
y <- X %*% beta_star + eps
# simulate data for y in logistic regression
pr <- 1 / (1 + exp(-X %*% beta_star))
y0 <- rbinom(n, 1, pr)
y1 <- y0
y1[which(y1 == 0)] <- -1
```

To perform the regularization path fitting at a sequence of regularization parameters, \textbf{sparsegl()} consumes aforementioned $\mX, \vY$, the grouping vector \textbf{group}, and other main arguments such that users can assign them with different values rather than follow the default: loss function indicator \textbf{family} could either assigned with "gaussian" or "binomial", a penalty vector \textbf{pf}, the weight of lasso penalty \textbf{asparse}, the lower and upper bounds for coefficient values \textbf{lower\_bnd} and \textbf{upper\_bnd}. This returns a \textbf{sparsegl} object and its associated attributes. One of the main attribute of this object, \textbf{df}, is a vector representing the number of nonzero coefficients at each $\lambda$ value and also an approximation to the exact degree-of-freedom.

```{r}
fit <- sparsegl(X, y, group = groups, family = "gaussian")
```

The affiliated functions to this fitted \textbf{sparsegl} object are \textbf{plot()},\textbf{coef()}, \textbf{predict()} and \textbf{print()}. Function \textbf{plot()} either produces a coefficient profile plot of the coefficient paths to $\{\lambda_s\}_{s=1}^n$, or a group-norm plot against the penalties scaled by the max penalty value, in terms of the assignments to the arguments. Functions \textbf{coef()} and \textbf{predict()} computes the coefficients and predictions $\hat{\vY}$ given a matrix $\tilde{\mX}$ at the requested $\lambda$s' respectively, and the selection for $\lambda$ values different than the used sequence at the fitting stage is also allowed. Function \textbf{print()} returns the number of nonzero features with nonzero coefficients at each $\lambda$.

```{r, results = "hide", fig.width = 4, fig.height = 2}
plot(fit, y_axis = "coef", x_axis = "penalty", add_legend = FALSE)
plot(fit, y_axis = "group", x_axis = "lambda", add_legend = FALSE)
coef(fit, s = c(0.02, 0.03))
predict(fit, newx = X[100,], x = fit$lambda[2:3])
print(fit)
```

This package also supports for conducting a k-fold cross-validation with \textbf{cv.sparsegl()} given $\mX, \vY$, a vector \textbf{group}, loss function indicator \textbf{family}, and an additional argument \textbf{pred.loss} indicating the loss to use for cross-validation error. This returns a fitted \textbf{cv.sparsegl} object. The affiliated functions are \textbf{plot()}, \textbf{coef()}, \textbf{predict()} for the \textbf{cv.sparsegl} object. Function \textbf{plot()} produces cross-validation curve with upper and lower confidence bounds plots for each $\lambda$ in the regularization path for the fitted \textbf{cv.sprasegl}. Functions \textbf{coef()} and \textbf{predict()} works similarly to those introduced above. The only differences are that they both consume \textbf{cv.sparsegl} objects, and the passed $\lambda$ values are also be \textbf{lambda.min} (the optimal $\lambda$ that gives the minimum mean cross-validation error) or \textbf{lambda.1se} (the largest $\lambda$ such that the error is within 1 standard error or the minimum).

```{r, results = "hide", fig.width = 4, fig.height = 2}
cv_fit <- cv.sparsegl(X, y, groups, family = "gaussian", pred.loss = "L2")
plot(cv_fit)
coef(cv_fit, s = c(0.02, 0.03))
predict(cv_fit, newx = X[50:60, ], s = "lambda.min")
```

To build up the logistic regression model, only several arguments are required to change based on how we use them from above:

```{r, results = "hide", fig.width = 4, fig.height = 2}
fit_logit <- sparsegl(X, y1, group = groups, family = "binomial")
plot(fit_logit, y_axis = "coef", x_axis = "penalty", add_legend = FALSE)
plot(fit_logit, y_axis = "group", x_axis = "lambda", add_legend = FALSE)
coef(fit_logit, s = c(0.02, 0.03))
predict(fit_logit, newx = X[100,], x = fit$lambda[2:3])
print(fit_logit)
cv_fit_logit <- cv.sparsegl(X, y1, groups, family = "binomial", pred.loss = "misclass")
plot(cv_fit_logit)
coef(cv_fit_logit, s = c(0.02, 0.03))
predict(cv_fit_logit, newx = X[50:60, ], s = "lambda.min")
```


For further detailed documentation and examples towards this package, it is recommended to look into this [website](https://dajmcdon.github.io/sparsegl/index.html).

# Comparisons with other packages

As we have stated in the beginning, \pkg{SGL} package is computationally inefficient when dealing with a high-dimensional optimization problem, while \pkg{gglasso} does not produce within-group sparsity during the modeling process. To verify that our package is time-saving and in the meanwhile, producing both within-group sparsity and between-group sparsity, we will repetitively generate regression datasets such that they include different sample sizes or a different number of features with or without within-group sparsity. In particular, we will evaluate and compare the time efficiency by \pkg{sparsegl}, \pkg{SGL} and \pkg{gglasso} for between-group sparsity cases, while \pkg{sparsegl}, \pkg{SGL} for including both between-group and within-group sparsity cases when the response variable is either continuous or binary. The elapsed time (in milliseconds) will be measured using \pkg{microbenchmark}, and the results will be visualized with boxplots.

The approach to simulating models is as follows: for each dataset, we first generate an input matrix $\mX$ by randomly sampling numbers from the standard normal distribution and converting them into a $n$-by-$p$ matrix, and the response vector depends on the regression model used in the optimization problem. For linear regression models, the vector $\vY$ is generated linearly by $\vY = \mX\vbeta^* + \boldsymbol\epsilon$, where the error term $\epsilon$ is also generated from the standard normal distribution as white noise. For logistic regression models, $vY$ is a vector containing binary values only. A vector of probabilities with length $n$, namely $\frac{1}{1 + \exp(-\mX\vbeta^*)}$ should be calculated, and each $y_i$ is randomly generated by Bernoulli distribution with the corresponding probability. The entries of coefficients vector $\vbeta^*$ from both models are zeros or nonzero, which is generated depending on whether within-group sparsity is considered. For the coefficients in the group $k$, all the entries of $\vbeta^{*(k)}$ are zeros or nonzeros if the grouping structure is not within-group sparse, otherwise, $\vbeta^{*(k)}$ could a subvector with a mixture of zero and nonzero values. We consider the following combinations of (n, p, within-group sparsity):

\begin{itemize}
\item $n = 100$, $p = 20, 60, 100, 200, 300, 500$ and within-group sparsity exists.
\item $p = 100$, $n = 20, 60, 100, 200, 300, 500$ and within-group sparsity exists.
\item $n = 100$, $p = 20, 60, 100, 200, 300, 500$ and within-group sparsity not exists.
\item $p = 100$, $n = 20, 60, 100, 200, 300, 500$ and within-group sparsity not exists.
\end{itemize}

For each combination, the elapsed time (in milliseconds) is recorded for fitting regularization path at 100 $\lambda$s' values by constructing models using \pkg{sparsegl}, \pkg{SGL} and \pkg{gglasso} in terms of the existence of within-group sparsity over 50 independent datasets. The boxplots illustrate the consumed time on each run.


Figure 1 incorporates all the cases above and plots the change in running time (in log scale) against different sample sizes $n$ or the number of features $p$. When implementing linear regression models, we notice that three packages have similar performance and consume approximately similar time when varying $n$ and $p$, but \pkg{SGL} always requires longer running time when moving to larger $n$ and $p$. In particular, the trajectories of \pkg{gglasso} and \pkg{sparsegl} are similar as shown in plots when not taking within-group sparsity into account. However, a remarkable improvement is demonstrated when building logistic regression models using \pkg{sparsegl}, which takes less time compared to both \pkg{gglasso} and \pkg{SGL}. The increment of time consumption is also comparably small when increasing $n$ and $p$ compared with the other two packages in this example. Overall, \pkg{sparsegl} makes a significant contribution as compared to the existing packages that address sparse group-lasso high-dimensional optimization problems, especially in the logistic regression.

```{r,echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.cap = "Running Time Comparison", fig.width = 5, fig.height = 9}
 source("helper_function.R")
 df <- readRDS("./running-time-records-current.rds")
 plot_box(df)
```

# Discussion and future work
We developed this package for solving sparse group lasso optimization problem based on \textbf{four-step algorithm} with a convex loss function and a combination of $l1$ and $l2$ penalties. This preserves the grouping structure for coefficients by producing both among- and within- group sparsity, and also makes a huge improvement on computational efficiency compared to other existing sparse group lasso packages, which is achieved by applying strong rule check and KKT check iteratively.


# Acknowledgements {.unnumbered}

Here we list all the packages that we needed to compile this code. \pkg{devtools} [@R-devtools] \pkg{usethis} [@R-usethis] \pkg{testthat} [@R-testthat] \pkg{knitr} [@R-knitr] \pkg{microbenchmark} [@R-microbenchmark] \pkg{ggplot2}[-@R-ggplot2] and \pkg{rticles} [-@R-rticles]. Also NSF and NSERC funding.
